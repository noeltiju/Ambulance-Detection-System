{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "siren_sounds = ['siren_sound_1.wav','siren_sound_2.wav','siren_sound_3.wav','traffic_sound_1.wav']\n",
    "traffic_sounds = ['traffic_sound_1.wav', 'traffic_sound_2.wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shwet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "import librosa\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import wave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm\n",
    "1) Start\n",
    "2) Detect the sound of Emergency Vehicle\n",
    "3) If the frequency matches then set the camera\n",
    "4) Capture the selected road image with vehicles\n",
    "5) Compare the image with the dataset.\n",
    "6) If there is match for emergency vehicle with the vehicle on the road then send the message to the DSS\n",
    "7) After receiving the message from smart object, DSS checks for the sender address.\n",
    "8) DSS takes the appropriate decision by clearing the lane traffic of the requesting smart object.\n",
    "9) If any new message from any other or same smart objects then go to step 7.\n",
    "Once the emergency vehicle passes away from the central junction (intersection), data will be added to cloud and normal routine\n",
    "of controlling the congestion will be carried out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_names</th>\n",
       "      <th>emergency_or_not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1503.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1420.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1764.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1356.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1117.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_names  emergency_or_not\n",
       "0    1503.jpg                 0\n",
       "1    1420.jpg                 0\n",
       "2    1764.jpg                 0\n",
       "3    1356.jpg                 0\n",
       "4    1117.jpg                 0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"Emergency_Vehicles/train.csv\")\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_names</th>\n",
       "      <th>emergency_or_not</th>\n",
       "      <th>img_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1503.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0.36862746, 0.3764706, 0.2901961], [0.38431...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1420.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0.84705883, 0.9137255, 0.9764706], [0.85098...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1764.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0.24313726, 0.2509804, 0.2], [0.22352941, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1356.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0.7647059, 0.84705883, 0.7176471], [0.62352...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1117.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0.0, 0.03137255, 0.0], [0.1254902, 0.160784...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_names  emergency_or_not  \\\n",
       "0    1503.jpg                 0   \n",
       "1    1420.jpg                 0   \n",
       "2    1764.jpg                 0   \n",
       "3    1356.jpg                 0   \n",
       "4    1117.jpg                 0   \n",
       "\n",
       "                                           img_array  \n",
       "0  [[[0.36862746, 0.3764706, 0.2901961], [0.38431...  \n",
       "1  [[[0.84705883, 0.9137255, 0.9764706], [0.85098...  \n",
       "2  [[[0.24313726, 0.2509804, 0.2], [0.22352941, 0...  \n",
       "3  [[[0.7647059, 0.84705883, 0.7176471], [0.62352...  \n",
       "4  [[[0.0, 0.03137255, 0.0], [0.1254902, 0.160784...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocessing images  - Image to array\n",
    "def preprocessing_img(file_path):\n",
    "    img = load_img(f\"Emergency_vehicles/train/{file_path}\",target_size=(224,224))\n",
    "    \n",
    "    img_array = img_to_array(img)\n",
    "    img_array /= 255.0\n",
    "    return img_array\n",
    "\n",
    "data_train[\"img_array\"] = data_train[\"image_names\"].apply(preprocessing_img)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing sound\n",
    "def preprocessing_sound(file_path,type):\n",
    "    if (type == 'siren'):\n",
    "        audio,sr = librosa.load(f'SirenSounds/{file_path}',sr = 44100)\n",
    "    else:\n",
    "        audio,sr = librosa.load(f'TrafficSounds/{file_path}',sr = 44100)\n",
    "\n",
    "    # Generate spectrogramq    ````````````````````````````\n",
    "    spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "    return spectrogram_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset development \n",
    "sound_column=[]\n",
    "new_column = []\n",
    "for index,row in data_train.iterrows():\n",
    "    if (row['emergency_or_not'] == 1):\n",
    "        file_path = choice(siren_sounds)\n",
    "        if (file_path[0] == 't'):\n",
    "            sound_column.append(preprocessing_sound(file_path=file_path,type='traffic'))\n",
    "            new_column.append(0)\n",
    "        else:\n",
    "            sound_column.append(preprocessing_sound(file_path=file_path,type='siren'))\n",
    "            new_column.append(1)\n",
    "    else:\n",
    "        file_path = choice(traffic_sounds)        \n",
    "        sound_column.append(preprocessing_sound(file_path=file_path,type = 'traffic'))\n",
    "        new_column.append(0)\n",
    "        \n",
    "data_train['siren_or_not'] = new_column\n",
    "data_train['sound'] = sound_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_names</th>\n",
       "      <th>emergency_or_not</th>\n",
       "      <th>img_array</th>\n",
       "      <th>siren_or_not</th>\n",
       "      <th>sound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1503.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0.36862746, 0.3764706, 0.2901961], [0.38431...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-14.50043, -11.244556, -11.512085, -10.55725...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1420.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0.84705883, 0.9137255, 0.9764706], [0.85098...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-14.50043, -11.244556, -11.512085, -10.55725...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1764.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0.24313726, 0.2509804, 0.2], [0.22352941, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-14.50043, -11.244556, -11.512085, -10.55725...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1356.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0.7647059, 0.84705883, 0.7176471], [0.62352...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-14.50043, -11.244556, -11.512085, -10.55725...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1117.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0.0, 0.03137255, 0.0], [0.1254902, 0.160784...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-28.686245, -27.850946, -19.184986, -15.9539...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_names  emergency_or_not  \\\n",
       "0    1503.jpg                 0   \n",
       "1    1420.jpg                 0   \n",
       "2    1764.jpg                 0   \n",
       "3    1356.jpg                 0   \n",
       "4    1117.jpg                 0   \n",
       "\n",
       "                                           img_array  siren_or_not  \\\n",
       "0  [[[0.36862746, 0.3764706, 0.2901961], [0.38431...             0   \n",
       "1  [[[0.84705883, 0.9137255, 0.9764706], [0.85098...             0   \n",
       "2  [[[0.24313726, 0.2509804, 0.2], [0.22352941, 0...             0   \n",
       "3  [[[0.7647059, 0.84705883, 0.7176471], [0.62352...             0   \n",
       "4  [[[0.0, 0.03137255, 0.0], [0.1254902, 0.160784...             0   \n",
       "\n",
       "                                               sound  \n",
       "0  [[-14.50043, -11.244556, -11.512085, -10.55725...  \n",
       "1  [[-14.50043, -11.244556, -11.512085, -10.55725...  \n",
       "2  [[-14.50043, -11.244556, -11.512085, -10.55725...  \n",
       "3  [[-14.50043, -11.244556, -11.512085, -10.55725...  \n",
       "4  [[-28.686245, -27.850946, -19.184986, -15.9539...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up sound_model for binary classification based on the sound.\n",
    "sound_model = Sequential()\n",
    "sound_model.add(Conv1D(32, 3, activation='relu', input_shape=(128, 259)))\n",
    "sound_model.add(MaxPooling1D(2))\n",
    "sound_model.add(Flatten())\n",
    "sound_model.add(Dense(128, activation='relu'))\n",
    "sound_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# sound_model = Sequential()\n",
    "# sound_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "# sound_model.add(MaxPooling2D((2, 2)))\n",
    "# sound_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# sound_model.add(MaxPooling2D((2, 2)))\n",
    "# sound_model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# sound_model.add(MaxPooling2D((2, 2)))\n",
    "# sound_model.add(Flatten())\n",
    "# sound_model.add(Dense(128, activation='relu'))\n",
    "# sound_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "52/52 [==============================] - 1s 8ms/step - loss: 4.8145 - accuracy: 0.8572\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.1034e-06 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 2.9092e-07 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 2.1082e-07 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1.8208e-07 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1.5943e-07 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1.4120e-07 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.2603e-07 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.1290e-07 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1.0155e-07 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f04a306450>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compling DataSet for Sound\n",
    "sound_model.compile(optimizer = \"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "#Fitting dataSets\n",
    "x_train_sound = np.array(data_train['sound'].tolist())\n",
    "y_train_sound = np.array(data_train['siren_or_not'].tolist())\n",
    "x_train_reshaped = x_train_sound.reshape(-1, 128, 259)\n",
    "sound_model.fit(x_train_sound,y_train_sound,epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up CNN for image modeling\n",
    "img_model = Sequential()\n",
    "img_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "img_model.add(MaxPooling2D((2, 2)))\n",
    "img_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "img_model.add(MaxPooling2D((2, 2)))\n",
    "img_model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "img_model.add(MaxPooling2D((2, 2)))\n",
    "img_model.add(Flatten())\n",
    "img_model.add(Dense(128, activation='relu'))\n",
    "img_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "52/52 [==============================] - 29s 536ms/step - loss: 0.7601 - accuracy: 0.6598\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 27s 517ms/step - loss: 0.4479 - accuracy: 0.7940\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 27s 513ms/step - loss: 0.3543 - accuracy: 0.8360\n",
      "Epoch 4/10\n",
      "52/52 [==============================] - 27s 512ms/step - loss: 0.2936 - accuracy: 0.8791\n",
      "Epoch 5/10\n",
      "52/52 [==============================] - 26s 497ms/step - loss: 0.2107 - accuracy: 0.9089\n",
      "Epoch 6/10\n",
      "52/52 [==============================] - 26s 501ms/step - loss: 0.1318 - accuracy: 0.9502\n",
      "Epoch 7/10\n",
      "52/52 [==============================] - 26s 495ms/step - loss: 0.0709 - accuracy: 0.9739\n",
      "Epoch 8/10\n",
      "52/52 [==============================] - 26s 504ms/step - loss: 0.0641 - accuracy: 0.9812\n",
      "Epoch 9/10\n",
      "52/52 [==============================] - 26s 504ms/step - loss: 0.0331 - accuracy: 0.9915\n",
      "Epoch 10/10\n",
      "52/52 [==============================] - 26s 493ms/step - loss: 0.0206 - accuracy: 0.9951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f065cadf90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compling DataSet for Image\n",
    "img_model.compile(optimizer = \"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "#Fitting dataSets\n",
    "x_train_img = np.array(data_train['img_array'].tolist())\n",
    "y_train_img = np.array(data_train['emergency_or_not'].tolist())\n",
    "img_model.fit(x_train_img,y_train_img,epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "52/52 [==============================] - 29s 539ms/step - loss: 0.6701 - accuracy: 0.7849\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 28s 530ms/step - loss: 0.2407 - accuracy: 0.8955\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 28s 548ms/step - loss: 0.1635 - accuracy: 0.9362\n",
      "Epoch 4/10\n",
      "52/52 [==============================] - 28s 529ms/step - loss: 0.0821 - accuracy: 0.9684\n",
      "Epoch 5/10\n",
      "52/52 [==============================] - 27s 525ms/step - loss: 0.0542 - accuracy: 0.9854\n",
      "Epoch 6/10\n",
      "52/52 [==============================] - 26s 505ms/step - loss: 0.0335 - accuracy: 0.9903\n",
      "Epoch 7/10\n",
      "52/52 [==============================] - 26s 501ms/step - loss: 0.0330 - accuracy: 0.9915\n",
      "Epoch 8/10\n",
      "52/52 [==============================] - 26s 499ms/step - loss: 0.0296 - accuracy: 0.9903\n",
      "Epoch 9/10\n",
      "52/52 [==============================] - 26s 501ms/step - loss: 0.0217 - accuracy: 0.9927\n",
      "Epoch 10/10\n",
      "52/52 [==============================] - 26s 498ms/step - loss: 0.0205 - accuracy: 0.9915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f0f232c510>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Extract feature vectors from both models\n",
    "sound_features = sound_model.layers[-2].output  # Last layer before sigmoid\n",
    "img_features = img_model.layers[-2].output\n",
    "y_train = np.array(data_train['siren_or_not'].tolist())\n",
    "# Concatenate the feature vectors\n",
    "combined_features = Concatenate()([sound_features, img_features])\n",
    "\n",
    "# Add additional layers for joint representation\n",
    "combined_features = Dense(128, activation='relu')(combined_features)\n",
    "combined_output = Dense(1, activation='sigmoid')(combined_features)\n",
    "\n",
    "# Create the combined model\n",
    "combined_model = Model(inputs=[sound_model.input, img_model.input], outputs=combined_output)\n",
    "\n",
    "# Compile the model\n",
    "combined_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the combined model using both sound and image data\n",
    "combined_model.fit([x_train_sound, x_train_img], y_train, epochs=10, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
